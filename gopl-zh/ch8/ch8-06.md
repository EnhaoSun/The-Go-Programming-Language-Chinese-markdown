示例: 并发的Web爬虫 · Go语言圣经

- [前言](../)- [Go语言起源](../ch0/ch0-01.md)
    - [Go语言项目](../ch0/ch0-02.md)
    - [本书的组织](../ch0/ch0-03.md)
    - [更多的信息](../ch0/ch0-04.md)
    - [致谢](../ch0/ch0-05.md)
- [入门](../ch1/ch1.md)- [Hello, World](../ch1/ch1-01.md)
    - [命令行参数](../ch1/ch1-02.md)
    - [查找重复的行](../ch1/ch1-03.md)
    - [GIF动画](../ch1/ch1-04.md)
    - [获取URL](../ch1/ch1-05.md)
    - [并发获取多个URL](../ch1/ch1-06.md)
    - [Web服务](../ch1/ch1-07.md)
    - [本章要点](../ch1/ch1-08.md)
- [程序结构](../ch2/ch2.md)- [命名](../ch2/ch2-01.md)
    - [声明](../ch2/ch2-02.md)
    - [变量](../ch2/ch2-03.md)
    - [赋值](../ch2/ch2-04.md)
    - [类型](../ch2/ch2-05.md)
    - [包和文件](../ch2/ch2-06.md)
    - [作用域](../ch2/ch2-07.md)
- [基础数据类型](../ch3/ch3.md)- [整型](../ch3/ch3-01.md)
    - [浮点数](../ch3/ch3-02.md)
    - [复数](../ch3/ch3-03.md)
    - [布尔型](../ch3/ch3-04.md)
    - [字符串](../ch3/ch3-05.md)
    - [常量](../ch3/ch3-06.md)
- [复合数据类型](../ch4/ch4.md)- [数组](../ch4/ch4-01.md)
    - [Slice](../ch4/ch4-02.md)
    - [Map](../ch4/ch4-03.md)
    - [结构体](../ch4/ch4-04.md)
    - [JSON](../ch4/ch4-05.md)
    - [文本和HTML模板](../ch4/ch4-06.md)
- [函数](../ch5/ch5.md)- [函数声明](../ch5/ch5-01.md)
    - [递归](../ch5/ch5-02.md)
    - [多返回值](../ch5/ch5-03.md)
    - [错误](../ch5/ch5-04.md)
    - [函数值](../ch5/ch5-05.md)
    - [匿名函数](../ch5/ch5-06.md)
    - [可变参数](../ch5/ch5-07.md)
    - [Deferred函数](../ch5/ch5-08.md)
    - [Panic异常](../ch5/ch5-09.md)
    - [Recover捕获异常](../ch5/ch5-10.md)
- [方法](../ch6/ch6.md)- [方法声明](../ch6/ch6-01.md)
    - [基于指针对象的方法](../ch6/ch6-02.md)
    - [通过嵌入结构体来扩展类型](../ch6/ch6-03.md)
    - [方法值和方法表达式](../ch6/ch6-04.md)
    - [示例: Bit数组](../ch6/ch6-05.md)
    - [封装](../ch6/ch6-06.md)
- [接口](../ch7/ch7.md)- [接口是合约](../ch7/ch7-01.md)
    - [接口类型](../ch7/ch7-02.md)
    - [实现接口的条件](../ch7/ch7-03.md)
    - [flag.Value接口](../ch7/ch7-04.md)
    - [接口值](../ch7/ch7-05.md)
    - [sort.Interface接口](../ch7/ch7-06.md)
    - [http.Handler接口](../ch7/ch7-07.md)
    - [error接口](../ch7/ch7-08.md)
    - [示例: 表达式求值](../ch7/ch7-09.md)
    - [类型断言](../ch7/ch7-10.md)
    - [基于类型断言识别错误类型](../ch7/ch7-11.md)
    - [通过类型断言查询接口](../ch7/ch7-12.md)
    - [类型分支](../ch7/ch7-13.md)
    - [示例: 基于标记的XML解码](../ch7/ch7-14.md)
    - [补充几点](../ch7/ch7-15.md)
- [Goroutines和Channels](ch8.md)- [Goroutines](ch8-01.md)
    - [示例: 并发的Clock服务](ch8-02.md)
    - [示例: 并发的Echo服务](ch8-03.md)
    - [Channels](ch8-04.md)
    - [并发的循环](ch8-05.md)
    - [示例: 并发的Web爬虫](ch8-06.md)
    - [基于select的多路复用](ch8-07.md)
    - [示例: 并发的目录遍历](ch8-08.md)
    - [并发的退出](ch8-09.md)
    - [示例: 聊天服务](ch8-10.md)
- [基于共享变量的并发](../ch9/ch9.md)- [竞争条件](../ch9/ch9-01.md)
    - [sync.Mutex互斥锁](../ch9/ch9-02.md)
    - [sync.RWMutex读写锁](../ch9/ch9-03.md)
    - [内存同步](../ch9/ch9-04.md)
    - [sync.Once惰性初始化](../ch9/ch9-05.md)
    - [竞争条件检测](../ch9/ch9-06.md)
    - [示例: 并发的非阻塞缓存](../ch9/ch9-07.md)
    - [Goroutines和线程](../ch9/ch9-08.md)
- [包和工具](../ch10/ch10.md)- [包简介](../ch10/ch10-01.md)
    - [导入路径](../ch10/ch10-02.md)
    - [包声明](../ch10/ch10-03.md)
    - [导入声明](../ch10/ch10-04.md)
    - [包的匿名导入](../ch10/ch10-05.md)
    - [包和命名](../ch10/ch10-06.md)
    - [工具](../ch10/ch10-07.md)
- [测试](../ch11/ch11.md)- [go test](../ch11/ch11-01.md)
    - [测试函数](../ch11/ch11-02.md)
    - [测试覆盖率](../ch11/ch11-03.md)
    - [基准测试](../ch11/ch11-04.md)
    - [剖析](../ch11/ch11-05.md)
    - [示例函数](../ch11/ch11-06.md)
- [反射](../ch12/ch12.md)- [为何需要反射?](../ch12/ch12-01.md)
    - [reflect.Type和reflect.Value](../ch12/ch12-02.md)
    - [Display递归打印](../ch12/ch12-03.md)
    - [示例: 编码S表达式](../ch12/ch12-04.md)
    - [通过reflect.Value修改值](../ch12/ch12-05.md)
    - [示例: 解码S表达式](../ch12/ch12-06.md)
    - [获取结构体字段标签](../ch12/ch12-07.md)
    - [显示一个类型的方法集](../ch12/ch12-08.md)
    - [几点忠告](../ch12/ch12-09.md)
- [底层编程](../ch13/ch13.md)- [unsafe.Sizeof, Alignof 和 Offsetof](../ch13/ch13-01.md)
    - [unsafe.Pointer](../ch13/ch13-02.md)
    - [示例: 深度相等判断](../ch13/ch13-03.md)
    - [通过cgo调用C代码](../ch13/ch13-04.md)
    - [几点忠告](../ch13/ch13-05.md)
- [附录](../appendix/appendix.html)- [附录A：原文勘误](../appendix/appendix-a-errata.html)
    - [附录B：作者译者](../appendix/appendix-b-author.html)
    - [附录C：译文授权](../appendix/appendix-c-cpoyright.html)
    - [附录D：其它语言](../appendix/appendix-d-translations.html)
- [本书使用 GitBook 发布](https://www.gitbook.com)

# [示例: 并发的Web爬虫](..)

## 8.6. 示例: 并发的Web爬虫

在5.6节中，我们做了一个简单的web爬虫，用bfs(广度优先)算法来抓取整个网站。在本节中，我们会让这个爬虫并行化，这样每一个彼此独立的抓取命令可以并行进行IO，最大化利用网络资源。crawl函数和gopl.io/ch5/findlinks3中的是一样的。

_gopl.io/ch8/crawl1_

```lang-go
func crawl(url string) []string {
    fmt.Println(url)
    list, err := links.Extract(url)
    if err != nil {
        log.Print(err)
    }
    return list
}

```

主函数和5.6节中的breadthFirst(广度优先)类似。像之前一样，一个worklist是一个记录了需要处理的元素的队列，每一个元素都是一个需要抓取的URL列表，不过这一次我们用channel代替slice来做这个队列。每一个对crawl的调用都会在他们自己的goroutine中进行并且会把他们抓到的链接发送回worklist。

```lang-go
func main() {
    worklist := make(chan []string)

    // Start with the command-line arguments.
    go func() { worklist <- os.Args[1:] }()

    // Crawl the web concurrently.
    seen := make(map[string]bool)
    for list := range worklist {
        for _, link := range list {
            if !seen[link] {
                seen[link] = true
                go func(link string) {
                    worklist <- crawl(link)
                }(link)
            }
        }
    }
}

```

注意这里的crawl所在的goroutine会将link作为一个显式的参数传入，来避免“循环变量快照”的问题（在5.6.1中有讲解）。另外注意这里将命令行参数传入worklist也是在一个另外的goroutine中进行的，这是为了避免channel两端的main goroutine与crawler goroutine都尝试向对方发送内容，却没有一端接收内容时发生死锁。当然，这里我们也可以用buffered channel来解决问题，这里不再赘述。

现在爬虫可以高并发地运行起来，并且可以产生一大坨的URL了，不过还是会有俩问题。一个问题是在运行一段时间后可能会出现在log的错误信息里的：

```
$ go build gopl.io/ch8/crawl1
$ ./crawl1 http://gopl.io/
http://gopl.io/
https://golang.org/help/
https://golang.org/doc/
https://golang.org/blog/
...
2015/07/15 18:22:12 Get ...: dial tcp: lookup blog.golang.org: no such host
2015/07/15 18:22:12 Get ...: dial tcp 23.21.222.120:443: socket: too many open files
...

```

最初的错误信息是一个让人莫名的DNS查找失败，即使这个域名是完全可靠的。而随后的错误信息揭示了原因：这个程序一次性创建了太多网络连接，超过了每一个进程的打开文件数限制，既而导致了在调用net.Dial像DNS查找失败这样的问题。

这个程序实在是太他妈并行了。无穷无尽地并行化并不是什么好事情，因为不管怎么说，你的系统总是会有一些个限制因素，比如CPU核心数会限制你的计算负载，比如你的硬盘转轴和磁头数限制了你的本地磁盘IO操作频率，比如你的网络带宽限制了你的下载速度上限，或者是你的一个web服务的服务容量上限等等。为了解决这个问题，我们可以限制并发程序所使用的资源来使之适应自己的运行环境。对于我们的例子来说，最简单的方法就是限制对links.Extract在同一时间最多不会有超过n次调用，这里的n一般小于文件描述符的上限值，比如20。这和一个夜店里限制客人数目是一个道理，只有当有客人离开时，才会允许新的客人进入店内。

我们可以用一个有容量限制的buffered channel来控制并发，这类似于操作系统里的计数信号量概念。从概念上讲，channel里的n个空槽代表n个可以处理内容的token（通行证），从channel里接收一个值会释放其中的一个token，并且生成一个新的空槽位。这样保证了在没有接收介入时最多有n个发送操作。（这里可能我们拿channel里填充的槽来做token更直观一些，不过还是这样吧。）由于channel里的元素类型并不重要，我们用一个零值的struct{}来作为其元素。

让我们重写crawl函数，将对links.Extract的调用操作用获取、释放token的操作包裹起来，来确保同一时间对其只有20个调用。信号量数量和其能操作的IO资源数量应保持接近。

_gopl.io/ch8/crawl2_

```lang-go
// tokens is a counting semaphore used to
// enforce a limit of 20 concurrent requests.
var tokens = make(chan struct{}, 20)

func crawl(url string) []string {
    fmt.Println(url)
    tokens <- struct{}{} // acquire a token
    list, err := links.Extract(url)
    <-tokens // release the token
    if err != nil {
        log.Print(err)
    }
    return list
}

```

第二个问题是这个程序永远都不会终止，即使它已经爬到了所有初始链接衍生出的链接。（当然，除非你慎重地选择了合适的初始化URL或者已经实现了练习8.6中的深度限制，你应该还没有意识到这个问题。）为了使这个程序能够终止，我们需要在worklist为空或者没有crawl的goroutine在运行时退出主循环。

```lang-go
func main() {
    worklist := make(chan []string)
    var n int // number of pending sends to worklist

    // Start with the command-line arguments.
    n++
    go func() { worklist <- os.Args[1:] }()

    // Crawl the web concurrently.
    seen := make(map[string]bool)

    for ; n > 0; n-- {
        list := <-worklist
        for _, link := range list {
            if !seen[link] {
                seen[link] = true
                n++
                go func(link string) {
                    worklist <- crawl(link)
                }(link)
            }
        }
    }
}

```

这个版本中，计数器n对worklist的发送操作数量进行了限制。每一次我们发现有元素需要被发送到worklist时，我们都会对n进行++操作，在向worklist中发送初始的命令行参数之前，我们也进行过一次++操作。这里的操作++是在每启动一个crawler的goroutine之前。主循环会在n减为0时终止，这时候说明没活可干了。

现在这个并发爬虫会比5.6节中的深度优先搜索版快上20倍，而且不会出什么错，并且在其完成任务时也会正确地终止。

下面的程序是避免过度并发的另一种思路。这个版本使用了原来的crawl函数，但没有使用计数信号量，取而代之用了20个常驻的crawler goroutine，这样来保证最多20个HTTP请求在并发。

```lang-go
func main() {
    worklist := make(chan []string)  // lists of URLs, may have duplicates
    unseenLinks := make(chan string) // de-duplicated URLs

    // Add command-line arguments to worklist.
    go func() { worklist <- os.Args[1:] }()

    // Create 20 crawler goroutines to fetch each unseen link.
    for i := 0; i < 20; i++ {
        go func() {
            for link := range unseenLinks {
                foundLinks := crawl(link)
                go func() { worklist <- foundLinks }()
            }
        }()
    }

    // The main goroutine de-duplicates worklist items
    // and sends the unseen ones to the crawlers.
    seen := make(map[string]bool)
    for list := range worklist {
        for _, link := range list {
            if !seen[link] {
                seen[link] = true
                unseenLinks <- link
            }
        }
    }
}

```

所有的爬虫goroutine现在都是被同一个channel - unseenLinks喂饱的了。主goroutine负责拆分它从worklist里拿到的元素，然后把没有抓过的经由unseenLinks channel发送给一个爬虫的goroutine。

seen这个map被限定在main goroutine中；也就是说这个map只能在main goroutine中进行访问。类似于其它的信息隐藏方式，这样的约束可以让我们从一定程度上保证程序的正确性。例如，内部变量不能够在函数外部被访问到；变量（§2.3.4）在没有发生变量逃逸（译注：局部变量被全局变量引用地址导致变量被分配在堆上）的情况下是无法在函数外部访问的；一个对象的封装字段无法被该对象的方法以外的方法访问到。在所有的情况下，信息隐藏都可以帮助我们约束我们的程序，使其不发生意料之外的情况。

crawl函数爬到的链接在一个专有的goroutine中被发送到worklist中来避免死锁。为了节省篇幅，这个例子的终止问题我们先不进行详细阐述了。

**练习 8.6：** 为并发爬虫增加深度限制。也就是说，如果用户设置了depth=3，那么只有从首页跳转三次以内能够跳到的页面才能被抓取到。

**练习 8.7：** 完成一个并发程序来创建一个线上网站的本地镜像，把该站点的所有可达的页面都抓取到本地硬盘。为了省事，我们这里可以只取出现在该域下的所有页面（比如golang.org开头，译注：外链的应该就不算了。）当然了，出现在页面里的链接你也需要进行一些处理，使其能够在你的镜像站点上进行跳转，而不是指向原始的链接。

**译注：**
拓展阅读 [Handling 1 Million Requests per Minute with Go](http://marcio.io/2015/07/handling-1-million-requests-per-minute-with-golang/)。

## 交流学习

如果有疑问或想和我交流，欢迎扫码下方二维码

![polarisxu](https://static.studygolang.com/static/img/polarisxu-qrcode-m.jpg?imageView2/2/w/154)

polarisxu

![gopherstudio](https://static.studygolang.com/static/img/gopherstudio2.png?imageView2/2/w/154)

gopherstudio

[Previous page: 并发的循环](ch8-05.md)[Next page: 基于select的多路复用](ch8-07.md)